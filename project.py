# -*- coding: utf-8 -*-
"""ProjectDS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19MB4O0kl7PNzILgW1baenV__pAJ5pbN2
"""

from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import warnings
import pickle

#Reading CSV file.
filename = Path().resolve() / 'data.csv'
data_frame = pd.read_csv(filename)
data_frame = data_frame.dropna()
data_frame.head()

data_frame = data_frame.drop(['url'], axis=1)

print(data_frame.head())

#Encoding phishing and legitimate to 1 and 0 respectively
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data_frame['status'] = le.fit_transform(data_frame['status'])
print(data_frame.head())



#Training model
#First Training and Testing data.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

def findMetrics(cm):
    print('Confusion Matrix : \n', cm)
    total1=sum(sum(cm))
    #####from confusion matrix calculate accuracy
    acc=(cm[0,0]+cm[1,1])/total1
    print("Accuracy: {0:.2%}".format(acc))
    sens = cm[0,0]/(cm[0,0]+cm[0,1])
    print("Sensitivity: {0:.2%}".format(sens))
    spec = cm[1,1]/(cm[1,0]+cm[1,1])
    print("Specifity: {0:.2%}".format(spec))

#Training model...
y = data_frame['status'].values
x = data_frame.drop(['status'], axis=1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

classifier = KNeighborsClassifier(n_neighbors=11)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)

findMetrics(confusion_matrix(y_test, y_pred))

from sklearn.utils import shuffle
data_frame = shuffle(data_frame)

#Correlation of each feature with the all other features.

corr = data_frame.corr()
#Save correlation in image
# from pandas.plotting import table

# ax = plt.subplot(111, frame_on=False) # no visible frame
# ax.xaxis.set_visible(False)  # hide the x axis
# ax.yaxis.set_visible(False)  # hide the y axis
# table(ax, corr)  # where df is your data frame

# plt.savefig('correlationTable.png')

def RemoveRelatedFeatures(dataset, th):
    corr_matrix = dataset.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > th)]
    dataset.drop(to_drop, axis=1, inplace=True)
    print("Columns removed from dataset: {}".format(to_drop))

print(x.shape)
RemoveRelatedFeatures(data_frame, 0.8)
data_frame.shape





#Checking whether data is balance or not.


# #Removing outliers using IQR
# for feature in data_frame.columns[0:10]:
#     Q1= data_frame[feature].quantile(0.25)
#     Q3 = data_frame[feature].quantile(0.75)
#     IQR = Q3 - Q1
#     upper_limit = Q3 + 1.5 * IQR
#     lower_limit = Q1 - 1.5 * IQR
#     data_frame = data_frame[(data_frame[feature] > lower_limit) & (data_frame[feature] < upper_limit)]
# print(data_frame.shape)

# red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')

# fig, axs = plt.subplots(1, len(data_frame[data_frame.columns[0:10]].columns), figsize=(20,10))

# for i, ax in enumerate(axs.flat):
#     ax.boxplot(data_frame.iloc[:,i], flierprops=red_circle)
#     ax.set_title(data_frame.columns[i], fontsize=20, fontweight='bold')
#     ax.tick_params(axis='y', labelsize=14)

# plt.savefig("BoxplotsAfterOutliers.png")
# plt.tight_layout()

#Boxplots
# cols = data_frame.columns[0:10]
# plt.rcParams["figure.figsize"] = [7.50, 3.50]
# plt.rcParams["figure.autolayout"] = True
# ax = data_frame.plot(kind='box', title='boxplot')
# plt.show()

#Histogram for dispersion of the data.


# for col in x.columns:
#     norm_col = np.log(data_frame[col])
#     norm_col.skew()
#     plt.hist(norm_col)
#     plt.hist(norm_col)

#Log transformation:
# col_1 = np.log(data_frame[cols[1]])
# col_1.skew()
# plt.hist(col_1)



#Applying normalization
columns_names = data_frame.columns[0:data_frame.shape[1]-1]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
# transform data
y = data_frame['status'].values
data_frame.drop(['status'], axis=1, inplace=True)

scaled = scaler.fit_transform(data_frame)
data_frame = pd.DataFrame(scaled, columns = columns_names)
data_frame['status'] = y
data_frame.head()

#Histograms after applying normalization.
# plt.rcParams["figure.figsize"] = [30, 30]
# scaled.hist(grid=False)
# plt.savefig("NormalizedHist.png")
# plt.show()



# #Removing outliers using IQR
# # data_frame.drop(["status"], axis=1, inplace=True)
# for feature in data_frame.columns[0:10]:
#     Q1= data_frame[feature].quantile(0.25)
#     Q3 = data_frame[feature].quantile(0.75)
#     IQR = Q3 - Q1
#     upper_limit = Q3 + 1.5 * IQR
#     lower_limit = Q1 - 1.5 * IQR
#     data_frame = data_frame[(data_frame[feature] > lower_limit) & (data_frame[feature] < upper_limit)]
# print(data_frame.shape)

data_frame.head()

#Training model with knnClassification

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

y = data_frame["status"].values
x = data_frame.drop(["status"], axis=1)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
k_values = []
acc_score = []
for k in range(3, 120, 2):
    k_values.append(k)
    classifier = KNeighborsClassifier(n_neighbors=k)
    classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    acc_score.append(acc)
    
ones = np.ones(len(k_values))
loss_graphValues = ones - acc_score
plt.figure(figsize=(15,5))
plt.xlabel("K Values")
plt.ylabel("Loss Graph value")
plt.plot(k_values, loss_graphValues, label='Loss Graph', marker='o')
plt.legend(bbox_to_anchor=(1, 1), bbox_transform=plt.gcf().transFigure)
plt.show()

plt.figure(figsize=(15,5))
plt.xlabel("K Values")
plt.ylabel("Accuracy")
plt.plot(k_values, acc_score, label='Accuracy Graph', marker='o')
plt.legend(bbox_to_anchor=(1, 1), bbox_transform=plt.gcf().transFigure)
plt.show()

y = data_frame["status"].values
x = data_frame.drop(["status"], axis=1)

#AUC --curve area value and graph to show

#Knn Classification

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
pred = knn.predict(x_test)
knn_prob = knn.predict_proba(x_test)[:,1]
conf_matrix = confusion_matrix(y_test, pred)
findMetrics(conf_matrix)
# pd.DataFrame(knn_prob).head()

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(x_train, y_train)
pred = gnb.predict(x_test)
gnb_prob = gnb.predict_proba(x_test)[:,1]

conf_matrix = confusion_matrix(y_test, pred)
findMetrics(conf_matrix)



from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)
pred = dtc.predict(x_test)
conf_matrix = confusion_matrix(y_test, pred)
findMetrics(conf_matrix)

dtc_prob = dtc.predict_proba(x_test)[:,1]
# pd.DataFrame(dtc_prob).head()

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
rfc.fit(x_train, y_train)
pred = rfc.predict(x_test)
conf_matrix = confusion_matrix(y_test, pred)
findMetrics(conf_matrix)
pickle.dump(rfc,open('rfcmodel.pkl','wb'))
rfc_prob = rfc.predict_proba(x_test)[:,1]

print(x_test.info())

